{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.layers import flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = 'train.p'\n",
    "validation_file= 'valid.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(X_train))\n",
    "Im_test = X_train[i]\n",
    "#print(np.shape(Im_test))\n",
    "#print(Im_test.min())\n",
    "\n",
    "print(y_train[i])\n",
    "\n",
    "Im_test = Im_test/Im_test.max()\n",
    "plt.figure()\n",
    "plt.imshow(Im_test)\n",
    "plt.show()\n",
    "print(Im_test.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(X_valid))\n",
    "Im_test = X_valid[i]\n",
    "\n",
    "imn = Im_test - np.mean(Im_test)\n",
    "imn = imn/(np.std(imn))\n",
    "#print(np.shape(imn))\n",
    "#print(np.max(imn))\n",
    "#print(np.min(imn))\n",
    "print(np.shape(imn))\n",
    "print(np.max(imn))\n",
    "print(np.min(imn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4410, 32, 32, 3)\n",
      "(4410,)\n",
      "(34799, 32, 32, 3)\n",
      "(34799,)\n"
     ]
    }
   ],
   "source": [
    "X_train_normalized = []\n",
    "y_train_normalized = []\n",
    "X_valid_normalized = []\n",
    "y_valid_normalized = []\n",
    "\n",
    "n = 0\n",
    "rotate_images = 0\n",
    "normalize_images = 1\n",
    "center_images = 0\n",
    "for n in range(len(X_train)):\n",
    "    Im = X_train[n]\n",
    "    if(normalize_images):\n",
    "        Im = Im/Im.max() - 0.5\n",
    "    if(center_images):\n",
    "        Imn = Im - np.mean(Im)\n",
    "        Im = Imn/(np.std(Imn))\n",
    "    lab = y_train[n]\n",
    "    \n",
    "    if(rotate_images):\n",
    "        i = random.randint(0,180)\n",
    "        M = cv2.getRotationMatrix2D((32/2,32/2),i,1)\n",
    "        dst = cv2.warpAffine(Im,M,(32,32))\n",
    "        im_transpose = dst\n",
    "    \n",
    "    if(rotate_images):\n",
    "        X_train_normalized.append(im_transpose)\n",
    "        X_train_normalized.append(Im)\n",
    "        y_train_normalized.append(lab)\n",
    "        y_train_normalized.append(lab)\n",
    "    else:\n",
    "        X_train_normalized.append(Im)\n",
    "        y_train_normalized.append(lab)\n",
    "        \n",
    "n = 0\n",
    "for n in range(len(X_valid)):\n",
    "    \n",
    "    Im = X_valid[n]\n",
    "    if(normalize_images):\n",
    "        Im = Im/Im.max() - 0.5\n",
    "    if(center_images):\n",
    "        Imn = Im - np.mean(Im)\n",
    "        Im = Imn/(Im.std(Imn))\n",
    "        \n",
    "    lab = y_valid[n]\n",
    "    \n",
    "    if(rotate_images):\n",
    "        i = random.randint(0,180)\n",
    "        M = cv2.getRotationMatrix2D((32/2,32/2),i,1)\n",
    "        dst = cv2.warpAffine(Im,M,(32,32))\n",
    "        im_transpose = dst\n",
    "    \n",
    "    if(rotate_images):\n",
    "        X_valid_normalized.append(im_transpose)\n",
    "        X_valid_normalized.append(Im)\n",
    "        y_valid_normalized.append(lab)\n",
    "        y_valid_normalized.append(lab)\n",
    "    else:\n",
    "        X_valid_normalized.append(Im)\n",
    "        y_valid_normalized.append(lab)\n",
    "\n",
    "print(np.shape(X_valid_normalized))\n",
    "print(np.shape(y_valid_normalized))\n",
    "\n",
    "print(np.shape(X_train_normalized))\n",
    "print(np.shape(y_train_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rrr_lenet(x):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x24.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 24), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(24))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x24. Output = 5x5x24.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 600. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(600, 200), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(200))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 200. Output = 100.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(200, 100), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(100))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 100. Output = 42.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(100, 42), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(42))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1_W = 0\n",
    "\n",
    "def rrr_test(x):\n",
    "    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Sub layer. Input = 28x28x12. Output = 14x14x12\n",
    "    #conv1a_W = tf.Variable(tf.truncated_normal(shape = (15,15,12,12), mean = mu, stddev = sigma))\n",
    "    #conv1a_b = tf.Variable(tf.zeros(12))\n",
    "    #conv1 = tf.nn.conv2d(conv1, conv1a_W, strides=[1,1,1,1],padding = 'VALID') + conv1_b\n",
    "    #covn1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x24.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 24), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(24))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 2400. Output = 1616.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(2400, 1200), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1200))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 1200. Output = 800.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1200, 800), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(800))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 800. Output = 42.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(800, 42), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(42))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rrr_test2(x):\n",
    "    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Sub layer. Input = 28x28x12. Output = 14x14x12\n",
    "    #conv1a_W = tf.Variable(tf.truncated_normal(shape = (15,15,12,12), mean = mu, stddev = sigma))\n",
    "    #conv1a_b = tf.Variable(tf.zeros(12))\n",
    "    #conv1 = tf.nn.conv2d(conv1, conv1a_W, strides=[1,1,1,1],padding = 'VALID') + conv1_b\n",
    "    #covn1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x24.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 24), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(24))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    conv2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 600. Output = 400.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(600, 400), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(400))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 400. Output = 200.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(400, 200), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(200))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 200. Output = 42.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(200, 42), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(42))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rrr_lenet2(x):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    #conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    conv1a_W = tf.Variable(tf.truncated_normal(shape = (15,15,12,12), mean = mu, stddev = sigma))\n",
    "    conv1a_b = tf.Variable(tf.zeros(12))\n",
    "    conv1a = tf.nn.conv2d(conv1,conv1a_W,strides = [1,1,1,1], padding = 'VALID') + conv1a_b\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1a)\n",
    "    \n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x24.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 24), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(24))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x24. Output = 5x5x24.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #conv2a_W = tf.Variable(tf.truncated_normal(shape = (6,6,24,24), mean = mu, stddev = sigma))\n",
    "    #conv2a_b = tf.Variable(tf.zeros(24))\n",
    "    #conv2a = tf.nn.conv2d(conv2,conv2a_W,strides = [1,1,1,1], padding = 'VALID') + conv2a_b\n",
    "    \n",
    "    #conv2 = tf.nn.relu(conv2a)\n",
    "    \n",
    "    # SOLUTION: Flatten. Input = 5x5x24. Output = 600.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 600. Output = 200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(600, 200), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(200))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 200. Output = 100.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(200, 100), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(100))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 100. Output = 42.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(100, 42), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(42))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rrr_lenet3(x):\n",
    "    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x16.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 16), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(16))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x16. Output = 14x14x32.\n",
    "    #conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    conv1a_W = tf.Variable(tf.truncated_normal(shape = (15,15,16,32), mean = mu, stddev = sigma))\n",
    "    conv1a_b = tf.Variable(tf.zeros(32))\n",
    "    conv1a = tf.nn.conv2d(conv1,conv1a_W,strides = [1,1,1,1], padding = 'VALID') + conv1a_b\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1a)\n",
    "    \n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x32.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 32), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(32))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x24. Output = 5x5x32.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #conv2a_W = tf.Variable(tf.truncated_normal(shape = (6,6,24,24), mean = mu, stddev = sigma))\n",
    "    #conv2a_b = tf.Variable(tf.zeros(24))\n",
    "    #conv2a = tf.nn.conv2d(conv2,conv2a_W,strides = [1,1,1,1], padding = 'VALID') + conv2a_b\n",
    "    \n",
    "    #conv2 = tf.nn.relu(conv2a)\n",
    "    \n",
    "    # SOLUTION: Flatten. Input = 5x5x32. Output = 800.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 600. Output = 200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(800, 400), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(400))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 400. Output = 126.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(400, 126), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(126))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 100. Output = 42.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(126, 42), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(42))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, drop_en):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    if(drop_en):\n",
    "        kp = 0.5\n",
    "    else:\n",
    "        kp = 1.0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BatchSize):\n",
    "        batch_x, batch_y = X_data[offset:offset + BatchSize], y_data[offset:offset + BatchSize]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: kp})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess)\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y,42)\n",
    "\n",
    "Epochs = 41\n",
    "BatchSize = 128\n",
    "\n",
    "rate = 0.00005 #0.00005 = Acc 0.90\n",
    "#rate = tf.Variable(0.0001)\n",
    "learning_rate = tf.placeholder(tf.float32, (None))\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#starter_learning_rate = 0.00025\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                           10, 0.96, staircase=True)\n",
    "logits = rrr_lenet3(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = one_hot_y, logits = logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation) #, global_step = global_step)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(one_hot_y,1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_processed_data = 1\n",
    "\n",
    "if(use_processed_data):\n",
    "    X_train = X_train_normalized\n",
    "    y_train = y_train_normalized\n",
    "    X_valid = X_valid_normalized\n",
    "    y_valid = y_valid_normalized\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print('Training...')\n",
    "    print()\n",
    "    \n",
    "    training_accuracy = 0.000\n",
    "    #rate = 0.1\n",
    "    for i in range(Epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        #if training_accuracy > 0.6:\n",
    "        #        rate = rate/2\n",
    "        for offset in range(0,num_examples,BatchSize):\n",
    "            end = offset + BatchSize\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            \n",
    "            sess.run(training_operation, feed_dict ={x: batch_x, y:batch_y, keep_prob :0.5})\n",
    "        \n",
    "        if i % 2 == 0 :\n",
    "            training_accuracy = evaluate(X_train, y_train,1)\n",
    "            validation_accuracy = evaluate(X_valid, y_valid,0)\n",
    "            \n",
    "            print('Epoch {} ....'.format(i+1))\n",
    "            print('Training Accuracy = {:.3f}'.format(training_accuracy))\n",
    "            print('Validation Accuracy = {:.3f}'.format(validation_accuracy))\n",
    "            print()\n",
    "        \n",
    "    #save_path = saver.save(sess, \"/rrr.ckpt\")    \n",
    "    saver.save(sess, './rrr_Lenet3_DropOut0p5_LR0p00005')\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imt = []\n",
    "\n",
    "# Test image 2\n",
    "Im_test = cv2.imread('TestImageBa_Stop.jpg')\n",
    "\n",
    "height, width = 32, 32\n",
    "res = cv2.resize(Im_test,(width, height), interpolation = cv2.INTER_CUBIC)\n",
    "b,g,r = cv2.split(res)\n",
    "img2 = cv2.merge([r,g,b])\n",
    "img2 = img2/img2.max() \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "img3 = np.reshape(img2 , newshape=(-1, 32, 32, 3))\n",
    "print(np.shape(img3))\n",
    "\n",
    "imt.append(img2)\n",
    "\n",
    "# Test image 2\n",
    "Im_test = cv2.imread('TestImageE_60KmHr.jpg')\n",
    "\n",
    "height, width = 32, 32\n",
    "res = cv2.resize(Im_test,(width, height), interpolation = cv2.INTER_CUBIC)\n",
    "b,g,r = cv2.split(res)\n",
    "img2 = cv2.merge([r,g,b])\n",
    "img2 = img2/img2.max() \n",
    "imt.append(img2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "# Test image 3\n",
    "Im_test = cv2.imread('TestImageF_70KmHr.jpg')\n",
    "\n",
    "height, width = 32, 32\n",
    "res = cv2.resize(Im_test,(width, height), interpolation = cv2.INTER_CUBIC)\n",
    "b,g,r = cv2.split(res)\n",
    "img2 = cv2.merge([r,g,b])\n",
    "img2 = img2/img2.max() \n",
    "imt.append(img2)\n",
    "\n",
    "# Test image 4\n",
    "Im_test = cv2.imread('TestImageB_Stop.jpg')\n",
    "\n",
    "height, width = 32, 32\n",
    "res = cv2.resize(Im_test,(width, height), interpolation = cv2.INTER_CUBIC)\n",
    "b,g,r = cv2.split(res)\n",
    "img2 = cv2.merge([r,g,b])\n",
    "img2 = img2/img2.max()\n",
    "imt.append(img2)\n",
    "plt.figure()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "# Test image 5\n",
    "Im_test = cv2.imread('TestImageG_120KmHr.jpg')\n",
    "\n",
    "height, width = 32, 32\n",
    "res = cv2.resize(Im_test,(width, height), interpolation = cv2.INTER_CUBIC)\n",
    "b,g,r = cv2.split(res)\n",
    "img2 = cv2.merge([r,g,b])\n",
    "img2 = img2/img2.max()\n",
    "imt.append(img2)\n",
    "plt.figure()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "print(np.shape(imt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized = []\n",
    "y_test_normalized = []\n",
    "\n",
    "n = 0\n",
    "rotate_images = 0\n",
    "normalize_images = 1\n",
    "center_images = 0\n",
    "for n in range(len(X_test)):\n",
    "    Im = X_test[n]\n",
    "    if(normalize_images):\n",
    "        Im = Im/Im.max() - 0.5\n",
    "    if(center_images):\n",
    "        Imn = Im - np.mean(Im)\n",
    "        Im = Imn/(np.std(Imn))\n",
    "    lab = y_test[n]\n",
    "    \n",
    "    if(rotate_images):\n",
    "        i = random.randint(0,180)\n",
    "        M = cv2.getRotationMatrix2D((32/2,32/2),i,1)\n",
    "        dst = cv2.warpAffine(Im,M,(32,32))\n",
    "        im_transpose = dst\n",
    "    \n",
    "    if(rotate_images):\n",
    "        X_test_normalized.append(im_transpose)\n",
    "        X_test_normalized.append(Im)\n",
    "        y_test_normalized.append(lab)\n",
    "        y_test_normalized.append(lab)\n",
    "    else:\n",
    "        X_test_normalized.append(Im)\n",
    "        y_test_normalized.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Key Variable_2/Adam not found in checkpoint\n\t [[Node: save/RestoreV2_33 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_33/tensor_names, save/RestoreV2_33/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_33', defined at:\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-2462a1ecec6f>\", line 26, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1000, in __init__\n    self.build()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key Variable_2/Adam not found in checkpoint\n\t [[Node: save/RestoreV2_33 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_33/tensor_names, save/RestoreV2_33/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable_2/Adam not found in checkpoint\n\t [[Node: save/RestoreV2_33 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_33/tensor_names, save/RestoreV2_33/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a6bc26efa008>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#saver = tf.train.import_meta_graph('rrr_Lenet3_DropOut0p5_LR0p00005.meta')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#saver.restore(sess, tf.train.latest_checkpoint('./'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./rrr_Lenet3_DropOut0p5_LR0p00005'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#predictions = tf.argmax(logits, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#check = predictions.eval(feed_dict = {x: imt, keep_prob: 1})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1386\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1388\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 964\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1014\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable_2/Adam not found in checkpoint\n\t [[Node: save/RestoreV2_33 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_33/tensor_names, save/RestoreV2_33/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_33', defined at:\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-2462a1ecec6f>\", line 26, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1000, in __init__\n    self.build()\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rranade\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key Variable_2/Adam not found in checkpoint\n\t [[Node: save/RestoreV2_33 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_33/tensor_names, save/RestoreV2_33/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:   \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    #saver = tf.train.import_meta_graph('rrr_Lenet3_DropOut0p5_LR0p00005.meta')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    saver.restore(sess, './rrr_Lenet3_DropOut0p5_LR0p00005')\n",
    "    #predictions = tf.argmax(logits, 1)\n",
    "    #check = predictions.eval(feed_dict = {x: imt, keep_prob: 1}) \n",
    "    #check = sess.run(logits, feed_dict = {x: imt, keep_prob: 1})\n",
    "    #values_op, indices_op = tf.nn.top_k(check[0], 5, sorted=True)\n",
    "    #print(sess.run(tf.nn.top_k(tf.constant(check[1]), k=1)))\n",
    "    #print(sess.run(tf.nn.top_k(tf.constant(check[3]), k=3)))\n",
    "    #print(check) #working\n",
    "    print(evaluate(X_test_normalized, y_test_normalized, 0)) #working\n",
    "    #print(sess.run(evaluate(X_test_normalized, y_test_normalized, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
